{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# utils\n",
    "from utils import get_embedding,load_embed,save_embed,data_preprocessing\n",
    "\n",
    "# data\n",
    "from data import myDS, mytestDS\n",
    "\n",
    "# model\n",
    "from model import Siamese_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'experiment_name': 'siamese-dropout_0.5',\n",
    "    'task': 'train',\n",
    "    'make_dict': False,\n",
    "    'data_preprocessing': False,\n",
    "\n",
    "    'ckpt_dir': 'ckpt/',\n",
    "\n",
    "    'training':{\n",
    "        'num_epochs': 20,\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer': 'sgd'\n",
    "    },\n",
    "    \n",
    "    'embedding':{\n",
    "        'full_embedding_path': 'input/wiki.es.vec',\n",
    "        'cur_embedding_path': 'input/embedding.pkl',\n",
    "    },\n",
    "        \n",
    "    'model':{\n",
    "        'fc_dim': 100,\n",
    "        'name': 'siamese',\n",
    "        'embed_size': 300,\n",
    "        'batch_size': 1,\n",
    "        'embedding_freeze': False,\n",
    "        'encoder':{\n",
    "            'hidden_size': 150,\n",
    "            'num_layers': 1,\n",
    "            'bidirectional': False,\n",
    "            'dropout': 0.5,\n",
    "        },  \n",
    "    },   \n",
    "    \n",
    "    'result':{\n",
    "        'filename':'result.txt',\n",
    "        'filepath':'res/',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "stops1 = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "def clean_sent(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(u'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]',' ',sent)\n",
    "    sent = re.sub('¡',' ',sent)\n",
    "    sent = re.sub('¿',' ',sent)\n",
    "    sent = re.sub('Á','á',sent)\n",
    "    sent = re.sub('Ó','ó',sent)\n",
    "    sent = re.sub('Ú','ú',sent)\n",
    "    sent = re.sub('É','é',sent)\n",
    "    sent = re.sub('Í','í',sent)\n",
    "    return sent\n",
    "def cleanSpanish(df):\n",
    "    df['spanish1'] = df.spanish1.map(lambda x: ' '.join([ word for word in\n",
    "                                                         nltk.word_tokenize(clean_sent(x).decode('utf-8'))]).encode('utf-8'))\n",
    "    df['spanish2'] = df.spanish2.map(lambda x: ' '.join([ word for word in\n",
    "                                                         nltk.word_tokenize(clean_sent(x).decode('utf-8'))]).encode('utf-8'))\n",
    "def removeSpanishStopWords(df, stop):\n",
    "\tdf['spanish1'] = df.spanish1.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x.decode('utf-8'))\n",
    "                                                         if word not in stop]).encode('utf-8'))\n",
    "\tdf['spanish2'] = df.spanish2.map(lambda x: ' '.join([word for word in nltk.word_tokenize(x.decode('utf-8'))\n",
    "                                                         if word not in stop]).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training data\n",
    "df_train_en_sp = pd.read_csv('./input/cikm_english_train_20180516.txt', sep='\t', header=None,\n",
    "                             error_bad_lines=False)\n",
    "df_train_sp_en = pd.read_csv('./input/cikm_spanish_train_20180516.txt', sep='\t', header=None,\n",
    "                             error_bad_lines=False)\n",
    "df_train_en_sp.columns = ['english1', 'spanish1', 'english2', 'spanish2', 'result']\n",
    "df_train_sp_en.columns = ['spanish1', 'english1', 'spanish2', 'english2', 'result']\n",
    "train1 = pd.DataFrame(pd.concat([df_train_en_sp['spanish1'], df_train_sp_en['spanish1']], axis=0))\n",
    "train2 = pd.DataFrame(pd.concat([df_train_en_sp['spanish2'], df_train_sp_en['spanish2']], axis=0))\n",
    "train_data = pd.concat([train1, train2], axis=1).reset_index()\n",
    "train_data = train_data.drop(['index'], axis=1)\n",
    "result = pd.DataFrame(pd.concat([df_train_en_sp['result'], df_train_sp_en['result']], axis=0)).reset_index()\n",
    "result = result.drop(['index'], axis=1)\n",
    "# pd.get_dummies(result['result']).head()\n",
    "train_data['result'] = result\n",
    "\n",
    "# Evaluation data\n",
    "test_data = pd.read_csv('./input/cikm_test_a_20180516.txt', sep='\t', header=None, error_bad_lines=False)\n",
    "test_data.columns = ['spanish1', 'spanish2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spanish1                  Donde está eso\n",
       "spanish2    Mi producto está defectuoso.\n",
       "Name: 1712, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[1712,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s1:那是哪里 where is that s2:我的产品是有缺陷的. via baidu翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spanish1                      no es eso\n",
       "spanish2    ¿Qué es Denunciar artículo?\n",
       "Name: 2349, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.iloc[2349,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s1: 不是这样的 it's not that; s2:什么是报案？ via baidu翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanSpanish(test_data)\n",
    "removeSpanishStopWords(test_data, stops1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirty sample count: 2\n"
     ]
    }
   ],
   "source": [
    "test_data.replace('', np.nan, inplace=True)\n",
    "dirty_data = test_data[test_data.isnull().any(axis=1)]\n",
    "print 'dirty sample count:', dirty_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling dirty test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spanish1</th>\n",
       "      <th>spanish2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>NaN</td>\n",
       "      <td>producto defectuoso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>NaN</td>\n",
       "      <td>denunciar artículo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     spanish1             spanish2\n",
       "1712      NaN  producto defectuoso\n",
       "2349      NaN   denunciar artículo"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirty_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.iloc[1712,0] = 'hola'\n",
    "test_data.iloc[2349,0] = 'hola'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.columns = ['s1', 's2']\n",
    "test_data.to_csv(\"input/cleaned_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('input/cleaned_train.csv')\n",
    "test_data = pd.read_csv('input/cleaned_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents = train_data['s1'].tolist() + train_data['s2'].tolist() + test_data['s1'].tolist() + test_data['s2'].tolist()\n",
    "\n",
    "testDS = mytestDS(test_data, all_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing embedding.\n"
     ]
    }
   ],
   "source": [
    "full_embed_path = config['embedding']['full_embedding_path']\n",
    "cur_embed_path = config['embedding']['cur_embedding_path']\n",
    "\n",
    "if os.path.exists(cur_embed_path) and not config['make_dict']:\n",
    "    embed_dict = load_embed(cur_embed_path)\n",
    "    print 'Loaded existing embedding.'\n",
    "else:\n",
    "    embed_dict = get_embedding(testDS.vocab._id2word, full_embed_path)\n",
    "    save_embed(embed_dict,cur_embed_path)\n",
    "    print 'Saved generated embedding.'\n",
    "\n",
    "\n",
    "vocab_size = len(embed_dict)\n",
    "# initialize nn embedding\n",
    "embedding = nn.Embedding(vocab_size, config['model']['embed_size'])\n",
    "embed_list = []\n",
    "for word in testDS.vocab._id2word:\n",
    "    embed_list.append(embed_dict[word])\n",
    "weight_matrix = np.array(embed_list)\n",
    "# pass weights to nn embedding\n",
    "embedding.weight = nn.Parameter(torch.from_numpy(weight_matrix).type(torch.FloatTensor), requires_grad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "config['embedding_matrix'] = embedding\n",
    "config['vocab_size'] = len(embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    " # model\n",
    "siamese = Siamese_lstm(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = config['training']['learning_rate']\n",
    "optimizer = torch.optim.SGD(filter(lambda x: x.requires_grad, siamese.parameters()) ,\n",
    "                                        lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: ckpt/siamese-dropout_0.5.pt\n"
     ]
    }
   ],
   "source": [
    "# Restore saved model (if one exists).\n",
    "ckpt_path = os.path.join(config['ckpt_dir'], config['experiment_name']+'.pt')\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    print('Loading checkpoint: %s' % ckpt_path)\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    epoch = ckpt['epoch']\n",
    "    siamese.load_state_dict(ckpt['siamese'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "else:\n",
    "    epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not shuffle here\n",
    "test_dataloader = DataLoader(dataset=testDS, num_workers=2, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    test_dataloader = DataLoader(dataset=testDS, num_workers=2, batch_size = 1)\n",
    "    prob_res = []\n",
    "    for idx, data in enumerate(test_dataloader, 0):\n",
    "\n",
    "        # get data\n",
    "        s1, s2 = data\n",
    "\n",
    "        # input \n",
    "        output = siamese(s1,s2)\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        res = sm(output.data)[:,1]\n",
    "        prob_res += res.data.tolist()\n",
    "    return prob_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Done.\n",
      "Result has writtn to res/result.txt , Good Luck!\n"
     ]
    }
   ],
   "source": [
    "result = inference()\n",
    "result = pd.DataFrame(result)\n",
    "print 'Inference Done.'\n",
    "res_path = os.path.join(config['result']['filepath'], config['result']['filename'])\n",
    "result.to_csv(res_path,header=False,index=False)\n",
    "print 'Result has writtn to', res_path, ', Good Luck!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

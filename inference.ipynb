{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from torch.autograd import Variable\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# utils\n",
    "from utils import get_embedding,load_embed,save_embed,data_preprocessing\n",
    "\n",
    "# data\n",
    "from data import myDS\n",
    "\n",
    "# model\n",
    "from model import Siamese_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'experiment_name': 'siamese-simple',\n",
    "    'task': 'train',\n",
    "    'make_dict': True,\n",
    "    'data_preprocessing': False,\n",
    "\n",
    "    'ckpt_dir': 'ckpt/',\n",
    "\n",
    "    'training':{\n",
    "        'num_epochs': 20,\n",
    "        'learning_rate': 0.003\n",
    "    },\n",
    "    \n",
    "    'embedding':{\n",
    "        'full_embedding_path': 'input/wiki.es.vec',\n",
    "        'cur_embedding_path': 'input/embedding.pkl',\n",
    "    },\n",
    "        \n",
    "\n",
    "    'model':{\n",
    "        'fc_dim': 100,\n",
    "        'name': 'siamese',\n",
    "        'embed_size': 300,\n",
    "        'batch_size': 1,\n",
    "        'embedding_freeze': False,\n",
    "        'encoder':{\n",
    "            'hidden_size': 150,\n",
    "            'num_layers': 1,\n",
    "            'bidirectional': False,\n",
    "            'dropout': 0.0,\n",
    "        },\n",
    "            \n",
    "        'decoder':{\n",
    "            'hidden_size': 150,\n",
    "            'num_layers': 1,\n",
    "            'bidirectional': False,\n",
    "            'dropout': 0.0,\n",
    "        },\n",
    "            \n",
    "    },   \n",
    "    \n",
    "    'result':{\n",
    "        'filename':'result.txt',\n",
    "        'filepath':'res/',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class mytestDS(Dataset):\n",
    "\n",
    "    def __init__(self, df, all_sents):\n",
    "        # Assign vocabularies.\n",
    "        self.s1 = df['s1'].tolist()\n",
    "        self.s2 = df['s2'].tolist()\n",
    "        self.vocab = Vocab(all_sents, sos_token='<sos>', eos_token='<eos>', unk_token='<unk>')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.s1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Split sentence into words.\n",
    "        s1_words = self.s1[idx].split()\n",
    "        s2_words = self.s2[idx].split()\n",
    "\n",
    "        # Add <SOS> and <EOS> tokens.\n",
    "        s1_words = [self.vocab.sos_token] + s1_words + [self.vocab.eos_token]\n",
    "        s2_words = [self.vocab.sos_token] + s2_words + [self.vocab.eos_token]\n",
    "\n",
    "        # Lookup word ids in vocabularies.\n",
    "        s1_ids = [self.vocab.word2id(word) for word in s1_words]\n",
    "        s2_ids = [self.vocab.word2id(word) for word in s2_words]\n",
    "\n",
    "        return s1_ids, s2_ids\n",
    "\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, all_sents, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n",
    "        \"\"\"Initialize the vocabulary.\n",
    "        Args:\n",
    "            iter: An iterable which produces sequences of tokens used to update\n",
    "                the vocabulary.\n",
    "            max_size: (Optional) Maximum number of tokens in the vocabulary.\n",
    "            sos_token: (Optional) Token denoting the start of a sequence.\n",
    "            eos_token: (Optional) Token denoting the end of a sequence.\n",
    "            unk_token: (Optional) Token denoting an unknown element in a\n",
    "                sequence.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.pad_token = '<pad>'\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "        # Add special tokens.\n",
    "        id2word = [self.pad_token]\n",
    "        if sos_token is not None:\n",
    "            id2word.append(self.sos_token)\n",
    "        if eos_token is not None:\n",
    "            id2word.append(self.eos_token)\n",
    "        if unk_token is not None:\n",
    "            id2word.append(self.unk_token)\n",
    "\n",
    "        # Update counter with token counts.\n",
    "        counter = Counter()\n",
    "        for x in all_sents:\n",
    "            counter.update(x.split())\n",
    "\n",
    "        # Extract lookup tables.\n",
    "        if max_size is not None:\n",
    "            counts = counter.most_common(max_size)\n",
    "        else:\n",
    "            counts = counter.items()\n",
    "            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
    "        words = [x[0] for x in counts]\n",
    "        id2word.extend(words)\n",
    "        word2id = {x: i for i, x in enumerate(id2word)}\n",
    "\n",
    "        self._id2word = id2word\n",
    "        self._word2id = word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    def word2id(self, word):\n",
    "        \"\"\"Map a word in the vocabulary to its unique integer id.\n",
    "        Args:\n",
    "            word: Word to lookup.\n",
    "        Returns:\n",
    "            id: The integer id of the word being looked up.\n",
    "        \"\"\"\n",
    "        if word in self._word2id:\n",
    "            return self._word2id[word]\n",
    "        elif self.unk_token is not None:\n",
    "            return self._word2id[self.unk_token]\n",
    "        else:\n",
    "            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n",
    "\n",
    "    def id2word(self, id):\n",
    "        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n",
    "        Args:\n",
    "            id: Integer id of the word being looked up.\n",
    "        Returns:\n",
    "            word: The corresponding word.\n",
    "        \"\"\"\n",
    "        return self._id2word[id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('input/cleaned_train.csv')\n",
    "test_data = pd.read_csv('input/cleaned_test.csv')\n",
    "\n",
    "all_sents = train_data['s1'].tolist() + train_data['s2'].tolist() + test_data['s1'].tolist() + test_data['s2'].tolist()\n",
    "\n",
    "testDS = mytestDS(test_data, all_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5142/5774 words with embedding vectors\n",
      "Missing Ratio: 10.95%\n",
      "Filled missing words' embeddings.\n",
      "Embedding Matrix Size:  5774\n",
      "Embedding saved\n",
      "Saved generated embedding.\n"
     ]
    }
   ],
   "source": [
    "full_embed_path = config['embedding']['full_embedding_path']\n",
    "cur_embed_path = config['embedding']['cur_embedding_path']\n",
    "\n",
    "if os.path.exists(cur_embed_path) and not config['make_dict']:\n",
    "    embed_dict = load_embed(cur_embed_path)\n",
    "    print 'Loaded existing embedding.'\n",
    "else:\n",
    "    embed_dict = get_embedding(testDS.vocab._id2word, full_embed_path)\n",
    "    save_embed(embed_dict,cur_embed_path)\n",
    "    print 'Saved generated embedding.'\n",
    "\n",
    "\n",
    "vocab_size = len(embed_dict)\n",
    "# initialize nn embedding\n",
    "embedding = nn.Embedding(vocab_size, config['model']['embed_size'])\n",
    "embed_list = []\n",
    "for word in testDS.vocab._id2word:\n",
    "    embed_list.append(embed_dict[word])\n",
    "weight_matrix = np.array(embed_list)\n",
    "# pass weights to nn embedding\n",
    "embedding.weight = nn.Parameter(torch.from_numpy(weight_matrix).type(torch.FloatTensor), requires_grad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "config['embedding_matrix'] = embedding\n",
    "config['vocab_size'] = len(embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " # model\n",
    "siamese = Siamese_lstm(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = config['training']['learning_rate']\n",
    "optimizer = torch.optim.SGD(filter(lambda x: x.requires_grad, siamese.parameters()) ,\n",
    "                                        lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: ckpt/siamese-simple.pt\n"
     ]
    }
   ],
   "source": [
    "# Restore saved model (if one exists).\n",
    "ckpt_path = os.path.join(config['ckpt_dir'], config['experiment_name']+'.pt')\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    print('Loading checkpoint: %s' % ckpt_path)\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    epoch = ckpt['epoch']\n",
    "    siamese.load_state_dict(ckpt['siamese'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "else:\n",
    "    epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not shuffle here\n",
    "test_dataloader = DataLoader(dataset=testDS, num_workers=2, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    test_dataloader = DataLoader(dataset=testDS, num_workers=2, batch_size = 1)\n",
    "    prob_res = []\n",
    "    for idx, data in enumerate(test_dataloader, 0):\n",
    "\n",
    "        # get data\n",
    "        s1, s2 = data\n",
    "\n",
    "        # input \n",
    "        output = siamese(s1,s2)\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        res = sm(output.data)[:,1]\n",
    "        prob_res += res.data.tolist()\n",
    "        if idx == 100: break\n",
    "    return prob_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Done.\n",
      "Result has writtn to res/result.txt , Good Luck!\n"
     ]
    }
   ],
   "source": [
    "result = inference()\n",
    "result = pd.DataFrame(result)\n",
    "print 'Inference Done.'\n",
    "res_path = os.path.join(config['result']['filepath'], config['result']['filename'])\n",
    "result.to_csv(res_path,header=False,index=False)\n",
    "print 'Result has writtn to', res_path, ', Good Luck!'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

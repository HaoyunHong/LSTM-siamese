{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# utils\n",
    "from utils import get_embedding, load_embed, save_embed, data_preprocessing\n",
    "# data\n",
    "from data import myDS, mytestDS\n",
    "# model\n",
    "from model import Siamese_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'experiment_name': 'siamese-transfer-baseline-1',\n",
    "    'task': 'train',\n",
    "    'make_dict': False,\n",
    "    'data_preprocessing': False,\n",
    "\n",
    "    'ckpt_dir': 'ckpt/',\n",
    "\n",
    "    'training':{\n",
    "        'num_epochs': 20,\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer': 'sgd'\n",
    "    },\n",
    "    \n",
    "    'embedding':{\n",
    "        'full_embedding_path': 'input/wiki.es.vec',\n",
    "        'cur_embedding_path': 'input/embedding.pkl',\n",
    "    },\n",
    "        \n",
    "    'model':{\n",
    "        'fc_dropout': 0.1,\n",
    "        'fc_dim': 100,\n",
    "        'name': 'siamese',\n",
    "        'embed_size': 300,\n",
    "        'batch_size': 1,\n",
    "        'embedding_freeze': False,\n",
    "        'encoder':{\n",
    "            'hidden_size': 150,\n",
    "            'num_layers': 1,\n",
    "            'bidirectional': False,\n",
    "            'dropout': 0.5,\n",
    "        },  \n",
    "    },   \n",
    "    \n",
    "    'result':{\n",
    "        'filename':'result.txt',\n",
    "        'filepath':'res/',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = pd.read_csv(\"input/cleaned_en.csv\")\n",
    "sp = pd.read_csv(\"input/cleaned_sp.csv\")\n",
    "test_data = pd.read_csv(\"input/cleaned_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "en.columns = ['s1', 's2', 'label']\n",
    "# split dataset\n",
    "msk = np.random.rand(len(en)) < 0.8\n",
    "en_train = en[msk]\n",
    "en_valid = en[~msk]\n",
    "en_all_sents = en['s1'].tolist() + en['s2'].tolist()\n",
    "\n",
    "# dataset\n",
    "en_trainDS = myDS(en_train, en_all_sents)\n",
    "en_validDS = myDS(en_valid, en_all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.columns = ['s1', 's2', 'label']\n",
    "# split dataset\n",
    "msk = np.random.rand(len(sp)) < 0.8\n",
    "sp_train = sp[msk]\n",
    "sp_valid = sp[~msk]\n",
    "sp_all_sents = sp['s1'].tolist() + sp['s2'].tolist()\n",
    "\n",
    "# dataset\n",
    "sp_trainDS = myDS(sp_train, sp_all_sents)\n",
    "sp_validDS = myDS(sp_valid, sp_all_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embed_dict = load_embed('input/en_embed.pkl')\n",
    "sp_embed_dict = load_embed('input/sp_embed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "en_embed_list = []\n",
    "for word in en_validDS.vocab._id2word:\n",
    "    en_embed_list.append(en_embed_dict[word])\n",
    "en_vocab_size = len(en_embed_list)\n",
    "    \n",
    "\n",
    "sp_embed_list = []\n",
    "for word in sp_trainDS.vocab._id2word:\n",
    "    sp_embed_list.append(sp_embed_dict[word])\n",
    "sp_vocab_size = len(sp_embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: English and Spanish embed list\n",
    "Output: English and Spanish aligned Embedding weight\n",
    "\"\"\"\n",
    "def align_embeddings(en_embed_list, sp_embed_list, embed_size):\n",
    "    print('English Vocab Size:{}, Spanish Vocab Size:{}'.format(len(en_embed_list), len(sp_embed_list)))\n",
    "    dif = abs(len(en_embed_list) - len(sp_embed_list))\n",
    "    compensate = []\n",
    "    for i in range(dif):\n",
    "        compensate.append(np.zeros(embed_size))\n",
    "    # shorter one aligned to longer one\n",
    "    if len(en_embed_list) < len(sp_embed_list):\n",
    "        en_embed_list.extend(compensate)\n",
    "    else: sp_embed_list.extend(compensate)\n",
    "    \n",
    "    if len(en_embed_list) == len(sp_embed_list):\n",
    "        print('-> Aligned to', len(en_embed_list))\n",
    "    \n",
    "    en_weight = nn.Parameter(torch.from_numpy(np.array(en_embed_list)).type(torch.FloatTensor), requires_grad = False)\n",
    "    sp_weight = nn.Parameter(torch.from_numpy(np.array(sp_embed_list)).type(torch.FloatTensor), requires_grad = False)\n",
    "\n",
    "    return en_weight, sp_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab Size:2685, Spanish Vocab Size:4101\n",
      "-> Aligned to 4101\n"
     ]
    }
   ],
   "source": [
    "aligned_size = max(en_vocab_size,sp_vocab_size)\n",
    "en_embedding = nn.Embedding(aligned_size, embed_size)\n",
    "sp_embedding = nn.Embedding(aligned_size, embed_size)\n",
    "\n",
    "en_embedding.weight, sp_embedding.weight = align_embeddings(en_embed_list, sp_embed_list, config['model']['embed_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese_lstm(\n",
      "  (encoder): LSTMEncoder(\n",
      "    (embedding): Embedding(4101, 300)\n",
      "    (lstm): LSTM(300, 150, dropout=0.5)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.1)\n",
      "    (1): Linear(in_features=600, out_features=100, bias=True)\n",
      "    (2): Tanh()\n",
      "    (3): Dropout(p=0.1)\n",
      "    (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liushijing/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "config['embedding_matrix'] = en_embedding\n",
    "# model\n",
    "siamese_en = Siamese_lstm(config)\n",
    "print(siamese_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: sgd\n",
      "Learning rate: 0.01\n",
      "Fresh start!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loss func\n",
    "loss_weights = Variable(torch.FloatTensor([1, 3]))\n",
    "if torch.cuda.is_available():\n",
    "    loss_weights = loss_weights.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss(loss_weights)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = config['training']['learning_rate']\n",
    "if config['training']['optimizer'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(filter(lambda x: x.requires_grad, siamese_en.parameters()), lr=learning_rate)\n",
    "elif config['training']['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, siamese_en.parameters()), lr=learning_rate)\n",
    "elif config['training']['optimizer'] == 'adadelta':\n",
    "    optimizer = torch.optim.Adadelta(filter(lambda x: x.requires_grad, siamese_en.parameters()), lr=learning_rate)\n",
    "elif config['training']['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(filter(lambda x: x.requires_grad, siamese_en.parameters()), lr=learning_rate)\n",
    "print('Optimizer:', config['training']['optimizer'])\n",
    "print('Learning rate:', config['training']['learning_rate'])\n",
    "\n",
    "# log info\n",
    "train_log_string = '%s :: Epoch %i :: Iter %i / %i :: train loss: %0.4f'\n",
    "valid_log_string = '%s :: Epoch %i :: valid loss: %0.4f\\n'\n",
    "\n",
    "# Restore saved model (if one exists).\n",
    "ckpt_path = os.path.join(config['ckpt_dir'], config['experiment_name']+'.pt')\n",
    "if os.path.exists(ckpt_path):\n",
    "    print('Loading checkpoint: %s' % ckpt_path)\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    epoch = ckpt['epoch']\n",
    "    siamese_en.load_state_dict(ckpt['siamese'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "else:\n",
    "    epoch = 1\n",
    "    print('Fresh start!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: siamese-transfer-baseline-1\n",
      "\n",
      "Start Epoch 1 Training...\n",
      "2018-07-28 19:39:39.277722 :: Epoch 1 :: Iter 5000 / 16942 :: train loss: 0.5665\n",
      "2018-07-28 19:40:52.057422 :: Epoch 1 :: Iter 10000 / 16942 :: train loss: 0.5506\n",
      "2018-07-28 19:42:04.341837 :: Epoch 1 :: Iter 15000 / 16942 :: train loss: 0.5155\n",
      "Train Loss at epoch 1: 0.5372381806373596\n",
      "\n",
      "Epoch 1 Validating...\n",
      "2018-07-28 19:42:58.845380 :: Epoch 1 :: valid loss: 0.4686\n",
      "\n",
      "Model saved!\n",
      "\n",
      "Start Epoch 2 Training...\n",
      "2018-07-28 19:44:13.207159 :: Epoch 2 :: Iter 5000 / 16942 :: train loss: 0.4723\n",
      "2018-07-28 19:45:27.205700 :: Epoch 2 :: Iter 10000 / 16942 :: train loss: 0.4560\n",
      "2018-07-28 19:46:41.484500 :: Epoch 2 :: Iter 15000 / 16942 :: train loss: 0.4342\n",
      "Train Loss at epoch 2: 0.4521958827972412\n",
      "\n",
      "Epoch 2 Validating...\n",
      "2018-07-28 19:47:36.285819 :: Epoch 2 :: valid loss: 0.4122\n",
      "\n",
      "Model saved!\n",
      "\n",
      "Start Epoch 3 Training...\n",
      "2018-07-28 19:48:50.928896 :: Epoch 3 :: Iter 5000 / 16942 :: train loss: 0.4003\n",
      "2018-07-28 19:50:05.003489 :: Epoch 3 :: Iter 10000 / 16942 :: train loss: 0.3979\n",
      "2018-07-28 19:51:21.824009 :: Epoch 3 :: Iter 15000 / 16942 :: train loss: 0.3895\n",
      "Train Loss at epoch 3: 0.39307355880737305\n",
      "\n",
      "Epoch 3 Validating...\n",
      "2018-07-28 19:52:16.838202 :: Epoch 3 :: valid loss: 0.3968\n",
      "\n",
      "Model saved!\n",
      "\n",
      "Start Epoch 4 Training...\n",
      "2018-07-28 19:53:31.875805 :: Epoch 4 :: Iter 5000 / 16942 :: train loss: 0.3600\n",
      "2018-07-28 19:54:47.554939 :: Epoch 4 :: Iter 10000 / 16942 :: train loss: 0.3634\n",
      "2018-07-28 19:56:02.840358 :: Epoch 4 :: Iter 15000 / 16942 :: train loss: 0.3335\n",
      "Train Loss at epoch 4: 0.3514634370803833\n",
      "\n",
      "Epoch 4 Validating...\n",
      "2018-07-28 19:56:57.481799 :: Epoch 4 :: valid loss: 0.3735\n",
      "\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train \"\"\"\n",
    "\n",
    "if config['task'] == 'train':\n",
    "\n",
    "    # save every epoch for visualization\n",
    "    train_loss_record = []\n",
    "    valid_loss_record = []\n",
    "    best_record = 10.0\n",
    "\n",
    "    # training\n",
    "    print('Experiment: {}\\n'.format(config['experiment_name']))\n",
    "\n",
    "    while epoch < config['training']['num_epochs']:\n",
    "\n",
    "        print('Start Epoch {} Training...'.format(epoch))\n",
    "\n",
    "        # loss\n",
    "        train_loss = []\n",
    "        train_loss_sum = []\n",
    "        # dataloader\n",
    "        train_dataloader = DataLoader(dataset=en_trainDS, shuffle=True, num_workers=2, batch_size=1)\n",
    "\n",
    "        for idx, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "            # get data\n",
    "            s1, s2, label = data\n",
    "\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # input\n",
    "            output = siamese_en(s1, s2)\n",
    "            output = output.squeeze(0)\n",
    "\n",
    "            # label cuda\n",
    "            label = Variable(label)\n",
    "            if torch.cuda.is_available():\n",
    "                label = label.cuda()\n",
    "\n",
    "            # loss backward\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.data.cpu())\n",
    "            train_loss_sum.append(loss.data.cpu())\n",
    "\n",
    "            # Every once and a while check on the loss\n",
    "            if ((idx + 1) % 5000) == 0:\n",
    "                print(train_log_string % (datetime.now(), epoch, idx + 1, len(en_train), np.mean(train_loss)))\n",
    "                train_loss = []\n",
    "\n",
    "        # Record at every epoch\n",
    "        print('Train Loss at epoch {}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
    "        train_loss_record.append(np.mean(train_loss_sum))\n",
    "\n",
    "        # Valid\n",
    "        print('Epoch {} Validating...'.format(epoch))\n",
    "\n",
    "        # loss\n",
    "        valid_loss = []\n",
    "        # dataloader\n",
    "        valid_dataloader = DataLoader(dataset=en_validDS, shuffle=True, num_workers=2, batch_size=1)\n",
    "\n",
    "        for idx, data in enumerate(valid_dataloader, 0):\n",
    "            # get data\n",
    "            s1, s2, label = data\n",
    "\n",
    "            # input\n",
    "            output = siamese_en(s1, s2)\n",
    "            output = output.squeeze(0)\n",
    "\n",
    "            # label cuda\n",
    "            label = Variable(label)\n",
    "            if torch.cuda.is_available():\n",
    "                label = label.cuda()\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(output, label)\n",
    "            valid_loss.append(loss.data.cpu())\n",
    "\n",
    "        print(valid_log_string % (datetime.now(), epoch, np.mean(valid_loss)))\n",
    "        # Record\n",
    "        valid_loss_record.append(np.mean(valid_loss))\n",
    "        epoch += 1\n",
    "\n",
    "        if np.mean(valid_loss)-np.mean(train_loss_sum) > 0.02:\n",
    "             print(\"Early Stopping!\")\n",
    "             break\n",
    "\n",
    "        # Keep track of best record\n",
    "        if np.mean(valid_loss) < best_record:\n",
    "            best_record = np.mean(valid_loss)\n",
    "            # save the best model\n",
    "            state_dict = {\n",
    "                'epoch': epoch,\n",
    "                'siamese': siamese_en.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state_dict, ckpt_path)\n",
    "            print('Model saved!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liushijing/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "config['experiment_name'] = config['experiment_name'] + '-Spain'\n",
    "config['embedding_matrix'] = sp_embedding\n",
    "siamese_sp = Siamese_lstm(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese_lstm(\n",
      "  (encoder): LSTMEncoder(\n",
      "    (embedding): Embedding(4101, 300)\n",
      "    (lstm): LSTM(300, 150, dropout=0.5)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.1)\n",
      "    (1): Linear(in_features=600, out_features=100, bias=True)\n",
      "    (2): Tanh()\n",
      "    (3): Dropout(p=0.1)\n",
      "    (4): Linear(in_features=100, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(siamese_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['training']['learning_rate'] = 0.01\n",
    "config['training']['optimizer'] = 'sgd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: sgd\n",
      "Learning rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "# loss func\n",
    "loss_weights = Variable(torch.FloatTensor([1, 3]))\n",
    "if torch.cuda.is_available():\n",
    "    loss_weights = loss_weights.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss(loss_weights)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = config['training']['learning_rate']\n",
    "if config['training']['optimizer'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(filter(lambda x: x.requires_grad, siamese_sp.parameters()), lr=learning_rate)\n",
    "elif config['training']['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, siamese_sp.parameters()), lr=learning_rate)\n",
    "elif config['training']['optimizer'] == 'adadelta':\n",
    "    optimizer = torch.optim.Adadelta(filter(lambda x: x.requires_grad, siamese_sp.parameters()), lr=learning_rate)\n",
    "elif config['training']['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(filter(lambda x: x.requires_grad, siamese_sp.parameters()), lr=learning_rate)\n",
    "print('Optimizer:', config['training']['optimizer'])\n",
    "print('Learning rate:', config['training']['learning_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 1\n",
    "# # log info\n",
    "# train_log_string = '%s :: Epoch %i :: Iter %i / %i :: train loss: %0.4f'\n",
    "# valid_log_string = '%s :: Epoch %i :: valid loss: %0.4f\\n'\n",
    "# best_record = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfering English Model from: ckpt/siamese-transfer-baseline.pt\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = 'ckpt/siamese-transfer-baseline.pt'\n",
    "print('Transfering English Model from: %s' % ckpt_path)\n",
    "ckpt = torch.load(ckpt_path)\n",
    "siamese_sp.load_state_dict(ckpt['siamese'])\n",
    "best_record = 10.0\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: siamese-transfer-baseline-1-Spain\n",
      "\n",
      "Start Epoch 1 Training...\n",
      "2018-07-29 09:30:08.443428 :: Epoch 1 :: Iter 5000 / 17058 :: train loss: 0.4356\n",
      "2018-07-29 09:31:29.370992 :: Epoch 1 :: Iter 10000 / 17058 :: train loss: 0.4223\n",
      "2018-07-29 09:32:51.388914 :: Epoch 1 :: Iter 15000 / 17058 :: train loss: 0.4143\n",
      "Train Loss at epoch 1: 0.420628160238266\n",
      "\n",
      "Epoch 1 Validating...\n",
      "2018-07-29 09:33:53.864678 :: Epoch 1 :: valid loss: 0.4088\n",
      "\n",
      "Model saved!\n",
      "\n",
      "Start Epoch 2 Training...\n",
      "2018-07-29 09:35:17.566606 :: Epoch 2 :: Iter 5000 / 17058 :: train loss: 0.3830\n",
      "2018-07-29 09:36:44.782698 :: Epoch 2 :: Iter 10000 / 17058 :: train loss: 0.3876\n",
      "2018-07-29 09:38:12.808182 :: Epoch 2 :: Iter 15000 / 17058 :: train loss: 0.3798\n",
      "Train Loss at epoch 2: 0.3844834864139557\n",
      "\n",
      "Epoch 2 Validating...\n",
      "2018-07-29 09:39:22.079134 :: Epoch 2 :: valid loss: 0.4039\n",
      "\n",
      "Model saved!\n",
      "\n",
      "Start Epoch 3 Training...\n",
      "2018-07-29 09:40:52.436446 :: Epoch 3 :: Iter 5000 / 17058 :: train loss: 0.3477\n",
      "2018-07-29 09:42:24.411405 :: Epoch 3 :: Iter 10000 / 17058 :: train loss: 0.3493\n",
      "2018-07-29 09:43:52.370767 :: Epoch 3 :: Iter 15000 / 17058 :: train loss: 0.3492\n",
      "Train Loss at epoch 3: 0.35114261507987976\n",
      "\n",
      "Epoch 3 Validating...\n",
      "2018-07-29 09:44:58.422772 :: Epoch 3 :: valid loss: 0.3951\n",
      "\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train \"\"\"\n",
    "\n",
    "if config['task'] == 'train':\n",
    "\n",
    "    # save every epoch for visualization\n",
    "    train_loss_record = []\n",
    "    valid_loss_record = []\n",
    "#     best_record = 10.0\n",
    "\n",
    "    # training\n",
    "    print('Experiment: {}\\n'.format(config['experiment_name']))\n",
    "\n",
    "    while epoch < config['training']['num_epochs']:\n",
    "\n",
    "        print('Start Epoch {} Training...'.format(epoch))\n",
    "\n",
    "        # loss\n",
    "        train_loss = []\n",
    "        train_loss_sum = []\n",
    "        # dataloader\n",
    "        train_dataloader = DataLoader(dataset=sp_trainDS, shuffle=True, num_workers=2, batch_size=1)\n",
    "\n",
    "        for idx, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "            # get data\n",
    "            s1, s2, label = data\n",
    "\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # input\n",
    "            output = siamese_sp(s1, s2)\n",
    "            output = output.squeeze(0)\n",
    "\n",
    "            # label cuda\n",
    "            label = Variable(label)\n",
    "            if torch.cuda.is_available():\n",
    "                label = label.cuda()\n",
    "\n",
    "            # loss backward\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.data.cpu())\n",
    "            train_loss_sum.append(loss.data.cpu())\n",
    "\n",
    "            # Every once and a while check on the loss\n",
    "            if ((idx + 1) % 5000) == 0:\n",
    "                print(train_log_string % (datetime.now(), epoch, idx + 1, len(sp_train), np.mean(train_loss)))\n",
    "                train_loss = []\n",
    "\n",
    "        # Record at every epoch\n",
    "        print('Train Loss at epoch {}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
    "        train_loss_record.append(np.mean(train_loss_sum))\n",
    "\n",
    "        # Valid\n",
    "        print('Epoch {} Validating...'.format(epoch))\n",
    "\n",
    "        # loss\n",
    "        valid_loss = []\n",
    "        # dataloader\n",
    "        valid_dataloader = DataLoader(dataset=sp_validDS, shuffle=True, num_workers=2, batch_size=1)\n",
    "\n",
    "        for idx, data in enumerate(valid_dataloader, 0):\n",
    "            # get data\n",
    "            s1, s2, label = data\n",
    "\n",
    "            # input\n",
    "            output = siamese_sp(s1, s2)\n",
    "            output = output.squeeze(0)\n",
    "\n",
    "            # label cuda\n",
    "            label = Variable(label)\n",
    "            if torch.cuda.is_available():\n",
    "                label = label.cuda()\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(output, label)\n",
    "            valid_loss.append(loss.data.cpu())\n",
    "\n",
    "        print(valid_log_string % (datetime.now(), epoch, np.mean(valid_loss)))\n",
    "        # Record\n",
    "        valid_loss_record.append(np.mean(valid_loss))\n",
    "        epoch += 1\n",
    "\n",
    "        if np.mean(valid_loss)-np.mean(train_loss_sum) > 0.02:\n",
    "             print(\"Early Stopping!\")\n",
    "             break\n",
    "\n",
    "        # Keep track of best record\n",
    "        if np.mean(valid_loss) < best_record:\n",
    "            best_record = np.mean(valid_loss)\n",
    "            # save the best model\n",
    "            state_dict = {\n",
    "                'epoch': epoch,\n",
    "                'siamese': siamese_sp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state_dict, ckpt_path)\n",
    "            print('Model saved!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'ckpt/siamese-transfer-baseline-sp.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert('aa')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
